{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
      "228813984/228813984 [==============================] - 195s 1us/step\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url,  cache_dir='.', untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('datasets/flower_photos')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making windows path\n",
    "import pathlib\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_images_dict = {\n",
    "    'roses': list(data_dir.glob('roses/*')),\n",
    "    'daisy': list(data_dir.glob('daisy/*')),\n",
    "    'dandelion': list(data_dir.glob('dandelion/*')),\n",
    "    'sunflowers': list(data_dir.glob('sunflowers/*')),\n",
    "    'tulips': list(data_dir.glob('tulips/*')),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making labels\n",
    "flowers_labels_dict = {\n",
    "    'roses': 0,\n",
    "    'daisy': 1,\n",
    "    'dandelion': 2,\n",
    "    'sunflowers': 3,\n",
    "    'tulips': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizing images\n",
    "X, y = [], []\n",
    "\n",
    "for flower_name, images in flowers_images_dict.items():\n",
    "    for image in images:\n",
    "        img = cv2.imread(str(image))\n",
    "        resized_img = cv2.resize(img,(180,180))\n",
    "        X.append(resized_img)\n",
    "        y.append(flowers_labels_dict[flower_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling images\n",
    "X_train_scaled = X_train / 255\n",
    "X_test_scaled = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "#data augmentation to improve accuracy\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
    "                                                 input_shape=(180, \n",
    "                                                              180,\n",
    "                                                              3)),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "86/86 [==============================] - 78s 838ms/step - loss: 1.4968 - accuracy: 0.3536\n",
      "Epoch 2/30\n",
      "86/86 [==============================] - 69s 800ms/step - loss: 1.1433 - accuracy: 0.5411\n",
      "Epoch 3/30\n",
      "86/86 [==============================] - 70s 815ms/step - loss: 1.0104 - accuracy: 0.5952\n",
      "Epoch 4/30\n",
      "86/86 [==============================] - 70s 812ms/step - loss: 0.9077 - accuracy: 0.6363\n",
      "Epoch 5/30\n",
      "86/86 [==============================] - 69s 806ms/step - loss: 0.8281 - accuracy: 0.6795\n",
      "Epoch 6/30\n",
      "86/86 [==============================] - 66s 766ms/step - loss: 0.7816 - accuracy: 0.6966\n",
      "Epoch 7/30\n",
      "86/86 [==============================] - 67s 776ms/step - loss: 0.7531 - accuracy: 0.7122\n",
      "Epoch 8/30\n",
      "86/86 [==============================] - 68s 786ms/step - loss: 0.6761 - accuracy: 0.7493\n",
      "Epoch 9/30\n",
      "86/86 [==============================] - 66s 767ms/step - loss: 0.6379 - accuracy: 0.7584\n",
      "Epoch 10/30\n",
      "86/86 [==============================] - 70s 808ms/step - loss: 0.6068 - accuracy: 0.7591\n",
      "Epoch 11/30\n",
      "86/86 [==============================] - 68s 796ms/step - loss: 0.5880 - accuracy: 0.7653\n",
      "Epoch 12/30\n",
      "86/86 [==============================] - 70s 817ms/step - loss: 0.5431 - accuracy: 0.7900\n",
      "Epoch 13/30\n",
      "86/86 [==============================] - 65s 758ms/step - loss: 0.5036 - accuracy: 0.8158\n",
      "Epoch 14/30\n",
      "86/86 [==============================] - 67s 780ms/step - loss: 0.4762 - accuracy: 0.8227\n",
      "Epoch 15/30\n",
      "86/86 [==============================] - 62s 721ms/step - loss: 0.4516 - accuracy: 0.8303\n",
      "Epoch 16/30\n",
      "86/86 [==============================] - 69s 803ms/step - loss: 0.4375 - accuracy: 0.8332\n",
      "Epoch 17/30\n",
      "86/86 [==============================] - 66s 766ms/step - loss: 0.3941 - accuracy: 0.8547\n",
      "Epoch 18/30\n",
      "86/86 [==============================] - 65s 750ms/step - loss: 0.3705 - accuracy: 0.8626\n",
      "Epoch 19/30\n",
      "86/86 [==============================] - 65s 753ms/step - loss: 0.3616 - accuracy: 0.8666\n",
      "Epoch 20/30\n",
      "86/86 [==============================] - 70s 813ms/step - loss: 0.3170 - accuracy: 0.8826\n",
      "Epoch 21/30\n",
      "86/86 [==============================] - 70s 817ms/step - loss: 0.3234 - accuracy: 0.8841\n",
      "Epoch 22/30\n",
      "86/86 [==============================] - 66s 773ms/step - loss: 0.3220 - accuracy: 0.8812\n",
      "Epoch 23/30\n",
      "86/86 [==============================] - 69s 800ms/step - loss: 0.3114 - accuracy: 0.8797\n",
      "Epoch 24/30\n",
      "86/86 [==============================] - 67s 785ms/step - loss: 0.2656 - accuracy: 0.9077\n",
      "Epoch 25/30\n",
      "86/86 [==============================] - 59s 686ms/step - loss: 0.2352 - accuracy: 0.9153\n",
      "Epoch 26/30\n",
      "86/86 [==============================] - 61s 711ms/step - loss: 0.2278 - accuracy: 0.9168\n",
      "Epoch 27/30\n",
      "86/86 [==============================] - 66s 767ms/step - loss: 0.2099 - accuracy: 0.9259\n",
      "Epoch 28/30\n",
      "86/86 [==============================] - 66s 769ms/step - loss: 0.2126 - accuracy: 0.9331\n",
      "Epoch 29/30\n",
      "86/86 [==============================] - 66s 766ms/step - loss: 0.2013 - accuracy: 0.9270\n",
      "Epoch 30/30\n",
      "86/86 [==============================] - 67s 782ms/step - loss: 0.1787 - accuracy: 0.9390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1743edb1810>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building cnn\n",
    "num_classes = 5\n",
    "\n",
    "model = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "model.fit(X_train_scaled, y_train, epochs=30)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 4s 120ms/step - loss: 1.0960 - accuracy: 0.7342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.096006155014038, 0.7342047691345215]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f426784fb6d7c4552ad99e551e7e1308dfd09add5e658d1a84ddf9d1ddbca18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
